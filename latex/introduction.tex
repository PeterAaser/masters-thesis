\chapter{Introduction}
\epigraph{They're trying to understand what space is. That's tough for them.
They break distances down into concentrations of chemicals. For them, space is a
range of taste intensities.}{Greg Bear, Blood Music}
%
Countless manhours have been spent improving the design and manufacturing
process of the digital computer, creating more and more complex architectures
capable of operating at ever greater speeds.
%
The brain is not subject to this top down design philosophy, yet through a
process of self organization neurons are capable of forming highly complex
networks capable of solving complex tasks, with far greater energy efficiency,
robustness and parallellism than any designed processor.
%
Inspired by work done in the field of material computing systems such as the
mecobo platform of nascence [citation needed], living neural networks grown from
human stem cells \emph{in vitro} on \emph{Micro Electrode Arrays} (MEA) are interfaced
with a digital computer, forming a hybrid neuro-digital system.
%
This system utilizes the theoretical framework of \emph{Reservoir Computing} to
help translate between the digital and biological parts of the system, allowing
it to solve simple tasks as a proof of concept.\\ \\
\section{Complexity}
In the 50's and 60's there was much optimism in the burgeoning field of
artificial intelligence. In 1965 H. A. Simon claimed ``machines will be capable,
within twenty years, of doing any work a man can
do.''\cite{vardi_artificial_nodate} , while Marvin Minsky boldly claimed in 1967
that ``Within a generation ... the problem of creating 'artificial intelligence'
will substantially be solved.'' \cite{noauthor_marvin_nodate}.
Had they chosen to predict any other field, such as logistics, information
sharing or communications their statements would have been prophetic and
visionary, so why did artificial intelligence turn out so differently?
%
To answer, consider the approaches the researchers employed:\\
The researchers sought to make machines that could use logic similar to that of
high level human thinking.
%
Therefore, it followed that the machine had to be programmed with rules
governing logic in order to reach sound conclusions.
%
To represent the prior and deduced knowledge, the researchers designed
programming languages such as lisp that could accurately describe these
operations.
%
In order to actually execute these lisp programs hardware had to be created,
supporting the primitive operations such as addition, subtraction and loading
from memory.
Regardless of the underlying platform a lisp program did not change meaning, and
the binary adder in the heart of the processor never interacted with the
floating point adder. In short, each piece of the puzzle was self contained,
and the \emph{complexity} of the system was similar to that of the sum of its
parts, which made it feasible to build large systems.
%
Nature, on the other hand, applies a completely different method.
Complex structures appear with no blueprint, arising from a process of
self-organization driven by a set of growth rules. This self organizing process
is capable of producing incredibly complex, robust and diverse structures whose
functionality arises not from specialized components working in isolation, but
from the interaction of many components.
%
Applying the superposition principle to the processor makes sense, it allows us
to study each individual component in isolation before investigating how they
interact.
%
On the other hand, applying the superposition principle to nature leaves us
blind to the fact that the purpose of most components is to interact rather than
performing a specific function.
\section{Computation}
The invention of the digital computer will be remembered as one of, if not the
most significant technological advances of mankind.
This is fitting, because it very clearly demonstrates the differences in
approach between the top down engineering approach of humans, and the self
organization of nature.
Since the components of a processor and a program is isolated and specialized it
is completely necessary that each component behaves reliably, as even the
slightest miscalculation can throw the whole system off balance.
Because of this a processor has to run all its instructions in an ordered
fashion, parallelism can at best be achieved by running sequential programs at
the same time.
In spite of these weaknesses, the digital computer is so ubiquitus
that other approaches have been dubbed \emph{Unconventional Computing}.
Unconvential computing, as implied by the name, comes in many forms such as
buckets of water \cite{fernando_pattern_2003}, or blobs of carbon nanotubes
[cite nascence]. 
These approaches seek to utilize the self organizing collective behaviors of
naturally occuring processes in physical systems, utilizing the interactions
that arise from the collective behavior of the system to perform calculations,
rather than the calculated sequential activation of specialized heterogenous
components of traditional digital computers.
% Introduces how computation occurs "naturally"
%-------------
As an example, consider the effort spent modelling and simulating snow [Cite
SIGGRAPH frozen paper] used in motion pictures such as Disney's frozen. Dozens
of machines in large rendering farms spend weeks rendering the snow in final
movie, however if you bring some dynamite and a helicopter nature will gladly
provide you with an avalanche ``for free''.\\
%
Toffoli argues that ``Nothing makes sense in computing except in the light of
evolution'' \cite{tommaso_toffoli_nothing_nodate}, 
%------------
%
%
Perhaps the crowning achievement of evolution is the human brain, capable of
performing vastly complex tasks, however only recently has understanding the
brain from a computational perspective become feasible.
\\ \\
The vast complexity of the human brain has made it a very difficult subject to
study and copy. Rather than understanding the human brain as a whole a more
feasible approach is to understand the underlying processes that allow neural
networks to self-organize into computationally capable networks.
In [Cite DeMarse flight controller] a neural network is grown in an MEA and
interfaced with a flight simulator. [Cite AHDNN] follows a similar approach,
using neurons to control a simple robot. The contribution presented in this
paper builds on this work, but adding RC...

\cleardoublepage

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End: