\chapter{Background}
\section{Complex Systems}
\emph{
  In this body of work references are made to computations done
  by both artificial and real neurons. To make the distinction
  between these cases clear all computation done by computer
  simulated approximations of neurons will be prefixed as
  artificial.
}\\
Creating a bridge between machine and neural culture is not
useful in itself without a way to interpret the electrical activity measured
from the network, and a way to encode information as electrical pulses to be
transmitted back to the network. To harness the computational power of neural
networks it is necessary to employ a simplified model of neurons which is capable of
describing how they self-organize into computationally capable networks, while leaving out
leaving out the implementation details.
Rather than starting with modeling neurons, which are, at least from our point of view, the
apex of evolution, simpler biological systems can be studied.
While nature exhibits a staggering diversity in both form and function there are
also many fundamental similarities, after all they have all emerged from
the same evolutionary processes.
Eco-systems, ant-hills and social networks all exhibit
complex non-linear behavior, where the global behavior of the system cannot be
traced back to a single part.
Natures acceptance of non-linearity fundamentally differs from products
engineered and designed by humans.
Our designs carefully contain complexity: We carefully design large systems out of
individual heterogonous components which can be understood in isolation,
creating clockworks where the complexity is a sum of parts.
In all of natures systems the behavior global behavior arises ultimately from
the interplay of homogenous components.
While these homogenous components may organize themselves into complex
specialized units such as the heart, these complex components not only consist
of homogenous cells, they are also assembled by locally interacting cells with
no blueprint, knowledge, or even an appreciation of the scale that the final
complex component will operate in.
This process of \emph{self organization} is by necessity highly robust. While
every heart has the same functionality, pumping blood, there are no two
identical hearts, even in identical twins there are variations, should you look
carefully enough.
%
% En setning her som knytter teksten over med den under. Rydd opp i teksten
% under litt for å passe med teksten over.
%
\\
In these \emph{Complex Systems} positive feedback can loops amplify small
perturbations into cascading effects, changing the entire system, while negative
feedback loops may cause other states to be relatively stable, resulting in
multiple meta-stable states, so called \emph{attractors}, that give the system some
measure of order.\\
% Setningen under lider litt av at enklere modeller allerede har blitt motivert
The first step in creating a theoretical framework for bridging the gap between
neuron and machine is then to study simpler bio-inspired models exhibiting this
complex behavior, such as \emph{cellular automata}.
A cellular automaton is a model of a single cell that will change its state based only on
its immediate neighbors. They are capable of solving global problems such
as contour-extraction \cite{sipper_emergence_1999}, establishing that local interactions can
produce interesting global behavior.\\
Cellular automata are even sufficiently powerful to express a turing machine,
but as Sipper puts it: ``This is perhaps the quintessential example of a slow
bullet train: embedding a sequential universal Turing machine within the
highly parallel cellular-automaton model.''
Embeddeding turing machines into cellular automatas is of little use, but it's
useful to know that cellular automata are sufficiently powerful if we are to
apply it as a model for the processes governing neural networks.
The real power of cellular automatas as a model for neural networks is how they
model the \emph{phase transitions} in behavior (i.e dynamics).
In Langton's pioneering paper \emph{Computation on the Edge of Chaos} \cite{langton_computation_1990} 
the system dynamics of cellular automata are shown to follow phase transitions
similar to physical matter.
Langton explored the rule space of cellular automata and found that the ratio
between transitions that led to cell death and life had similarities to
temperature in physical systems.
As expected, rules which tended to favor cell death led to static or periodic
systems, while rules favoring life over death led to chaotic systems.
More interstingly is what happened when the rules favored life and death
equally.
In these systems which exists at the border between orderly and chaotic systems
Langton found a \emph{critical} phase where the system was neither chaotic nor
ordered.
It is important to note that Langton did not seek to solve a specific problem
with his automatons, but to explore which automatas capable of supporting
universal computation, hypothesized by Wolfram \cite{wolfram_universality_1984}.
Criticality applies to any dynamic system, not just cellular automata, and the
study of adaptive networks \cite{sayama_modeling_2013} suggests that many
systems exhibit a homeostatic regulation of system dynamics to ensure that it
stays in the critical phase, including neurons
\cite{bornholdt_topological_2000}.
\section{Reservoir Computing}
The theory behind complex systems provides us with a useful model of biological
computation with emphasis on how a \emph{computational substrate} can be
achieved.
If the fundamental purpose of a neuron is to create networks exhibiting the same
complexity behavior as Langton's automatas then harnessing the computational
capabilities of these simple models is a first step towards interfacing and
understanding neurons.
Clearly designing a cellular computer in a top down manner is intractable due to
the intricate and unpredictable relation between cause and effect.
The best we could realistically achieve with the typical top down approach is
implementing a turing machine which would be both slow and incredibly brittle.
Seemingly, classifying the neural culture as a complex system has not provided
any useful tools for understanding how to interact with it, on the other hand it
makes this task seem futile.
However, in computer science the recent field of
\emph{reservoir computing} has emerged, embracing the complexity and unpredictability
of certain complex systems.
In reservoir computing, a complex systems is used as a \textit{reservoir}
\cite{schrauwen_overview_2007} which
``acts as a complex nonlinear dynamic filter that transforms the
input signals using a high-dimensional temporal map, not unlike the operation
of an explicit, temporal kernel function.''\\

% cite on SVMs?
In order to explain, schrauwen makes a comparison to how source
vector machines work: The reservoir acts as a kernel, projecting input into a
high-dimensional feature space.
Figure ??? shows this technique, note that the regression performed upon the
feauture space is a simple linear regression, an important point both in SVMs
and reservoir computing.

% ??? figure skal være fra schrauwen
Figure ??? shows a typical reservoir computing setup which follows a similar
method of operation as the SVM in figure ???.
The reservoir serves as the high dimensional feature space, while the output
layer is only capable of linearly separating the resulting dynamics.

Schrauwen points out two major differences between SVMs and RCs.
First, SVMs only implicitly expands the input to high dimensional space in order
to make the problem tractable, while reservoirs do not.
Secondly, kernels are not capable of handling temporal signals.

The second difference is very important, it is what allows reservoirs to
implicitly encode temporal signals in their dynamics, making reservoirs a
natural fit for tasks such as speech recognition.
% cite Biologically Plausible Speech Recognition with LSTM Neural Nets Alex
% Graves, Douglas Eck, Nicole Beringer, Juergen Schmidhuber

In other terms, the properties that make complex systems so hard to work with
such as sensitivity to initial conditions also allow them to discern very subtle
nuances in input, and their complex behavioral patterns causes the systems to
change their behavior to new input based on previous input.

In light of this, asking how to build a computer using Langton's automatons is
the wrong question, instead the focus should be on how exploit the computation
that is already occuring.\\
There are many examples of reservoirs which have been successfully exploited:
In \cite{jaeger_adaptive_2003} an \textit{echo state network} 
is utilized to solve classification problems.
More esoteric reservoirs have been used, for instance in
\cite{natschlager_liquid_2002} the idea of reservoir computing is taken quite
literally using a bucket of water as a reservoir.\\

\subsection{Linear and nonlinear output layers}

\section{Evolution In Materio}
In the previous section no distinction has been made between systems that exist
in the simulated world of a computer, and the physically occuring systems found
in nature.
Even in the early days of computing however, long before reservoir computing,
research into the computational capabilities inherent in physical matter.
% ratio club etc

\section{Neurons As Computers}
% Hva jeg vil få frem:
% En kjapp intro av nevroner
% Hvordan jeg ser nevroner som enkle noder som kommuniserer elektrisk uten å ta
% hensyn til det underliggende elektrokjemiske systemet.
% Hvordan nevroner selv-organiserer seg, og hvordan struktur påvirke adferd, som
% igjen påvirker videre vekst (noe som er veldig veldig interessant!!)
% Beskrive hvordan spiking fører til kjedereaksjoner
% Beskrive hvordan vi kan koble oss på nevronene vha elektriske signaler.
Even a single neuron is an extremely complex system when compared to the
transistor. Together they form vast interconnected networks where information is
exchanged using electrochemical signals. A fundamental difference between
neurons and conventional computers is that these networks have not been
designed, they do not come with a blueprint, and they constantly modify
themselves in response to the environment. Due to the complexities of this
process it is completely necessary to restrict ourselves to a simple model of
the neuron, leaving genetic activations and chemical pathways to the
neurologists and chemists. In order to understand how neurons can assemble into
complex networks capable of thought one must of course understand the underlying
chemical and biological principles, but from a computer scientists perspective
these are mere implementation details necessitated by existing in a physical
universe. Were the physical laws altered the chemical pathways would surely
differ, but the resulting behavior would probably [According to who?] still be
similar to our universe. In this viewpoint, the very essence of neurons is their
ability to self-organize into structures in a bottom-up fashion in a complex
interplay between behavior and form. Following this rationale the neuron will be
seen as a simple node in a network, communicating using electrical pulses,
so-called spikes or action potentials. These spikes are short bursts of
electricity which after firing causes a quick refactory period in which the
firing cell will not respond to stimuli.
When fired these spikes stimulates connected neurons, which may in turn
release their charge causing a signal to cascade in a feedback loop.
These electrical pulses can be seen as the ``language'' of neurons, and by
measuring electrical activity and stimulating the network using electrodes, it
is possible to set up two-way communication between a machine and a cluster of
neurons.

\section{}
\cleardoublepage

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End: