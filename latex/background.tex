\chapter{Background}
\emph{
  In this body of work references are made to computations done
  by both artificial and real neurons. To make the distinction
  between these cases clear all computation done by computer
  simulated approximations of neurons will be prefixed as
  artificial.
}\\
The underlying goal of the thesis, performing computations with neurons serves
as a red thread throughout the background chapter.
To create a cyborg, it is necessery to create a bridge between machine and
neural cultures.
This bridge must provide a medium which can be understood by both neural tissue
and the computer, analogous to how humans communicate with soundwaves and visual
cues.
Once a medium has been established, the real challenge presents itself, namely
that of finding a common ``language''.
The topics discussed in this section serves to motivate how this goal can be
achieved is structured as follows:
First \emph{Complex Systems} are introduced as a framework to discuss the computational
capabilities in a wide range of systems that exhibit system dynamics similar to
that of neurons.
Next, \emph{Evolution in Materio}, EiM for short, introduces computation done in
unstructured matter through the process of evolution.
The goals of EiM are closely aligned to the goal of this thesis, as both study
massively parallel computation happening in physical matter shaped by the
process of evolution.
Next section, \emph{Neurons As Computers}, gives a basic overview of neurons,
and more importantly motivates narrowing the scope of how neurons will be
modeled, a necessity considering the size of the field and the background of the author.
Finally, \emph{Reservoir Computing} is introduced, tying together the previous
sections by introducing a framework of thought capable of tapping into the
computational properties of neural networks.

\section{Complex Systems}
TODO: Introduser Adaptive networks, siter Sayama: "Modeling complex systems with
adaptive networks"\\ \\
%
Nature, unlike humans, does not shy away from complexity.
Since complexity in itself is not the goal of evolution (evolution has no goal), 
it is interesting to note that systems that arise from a process of evolution
display much more complexity than human-designed systems, and the study of these
systems provide useful models and terminology for describing and understanding
how neural networks form.
%
The study of these \emph{Complex Systems} is a broad, cross-disciplinary field.
To see why, consider that most fields deal with systems, be they financial,
chemical, social or electrical in nature.
All of these fields feature systems in which the global behavior \emph{Emerges}
from local interaction of individual components through a process of \emph{Self
  Organization}.
These behavior of these systems exhibit a complexity that is greater than the
sum of complexity of its individual components, i.e the relationship between the
complexity of the system and its constituent components is \emph{Non Linear}.
Common for these systems is therefore the futility of reductionism since the
interesting aspects of the system only occurs when components interact.
Although complex systems are not ordered or linear, they are not chaotic either.
%
Unlike chaotic systems, complex systems contain \emph{Attractors}, states that
are relatively stable.
Fig [Strange attractor] shows the \emph{Strange Attractor}, a pattern formed by
a particle in a vector field that orbits two attractors, arbitrarily switching
between them.
In complex systems both dampening and amplifying feedback loops form.
The dampening loops allow the system to fall into the attractors, while the
amplifying loops can amplify minor perturbations, eventually causing a cascading
feedback loop moving the system to a different attractor.\\
%
The immediate benefit of classifying neural cultures as a complex systems is
that studying simpler models of complex systems can act as a stepping stone.
A simple, biological inspired model capable of exhibiting complex behavior is
the \emph{Cellular Automaton}, a discrete model of a single cell which changes
between a discrete set of states based only on its immediate neighbors.
Together they are capable of solving global problems such
as contour-extraction \cite{sipper_emergence_1999}, providing an example of
local interactions producing interesting global behavior.\\
Cellular automata are even sufficiently powerful to express a turing machine,
but as Sipper puts it: ``This is perhaps the quintessential example of a slow
bullet train: embedding a sequential universal Turing machine within the
highly parallel cellular-automaton model.''
Embeddeding turing machines into cellular automatas is of little use, but it's
useful to know that cellular automata are sufficiently powerful if we are to
apply it as a model for the processes governing neural networks.
The real power of cellular automatas as a model for neural networks is how they
model the \emph{Phase Transitions} in behavior (i.e dynamics).
In Langton's pioneering paper \emph{Computation on the Edge of Chaos}
\cite{langton_computation_1990}
the system dynamics of cellular automata are shown to follow phase transitions
similar to physical matter.
Langton explored the rule space of cellular automata and found that the ratio
between transitions that led to cell death and life had similarities to
temperature in physical systems.
As expected, rules which tended to favor cell death led to static or periodic
systems, while rules favoring life over death led to chaotic systems.
More interstingly is what happened when the rules favored life and death
equally.
In these systems which exists at the border between orderly and chaotic systems
Langton found a \emph{critical} phase where the system was neither chaotic nor
ordered.
It is important to note that Langton did not seek to solve a specific problem
with his automatons, but to explore which automatas capable of supporting
universal computation, hypothesized by Wolfram \cite{wolfram_universality_1984}.
Criticality applies to any dynamic system, not just cellular automata, and the
study of adaptive networks \cite{sayama_modeling_2013} suggests that many
systems exhibit a homeostatic regulation of system dynamics to ensure that it
stays in the critical phase, including neurons
\cite{bornholdt_topological_2000}.

\section{Evolution In Materio}
Classifying neurons as a complex system has not gotten us closer to a solution
for interfacing with them in a meaningful way it seems.
However, one useful conclusion is that neurons work nothing like human designed
conventional computers, and that to understand how neurons compute we should
look at more \emph{unconventional computing}, specifically work done on physical
matter.
Two pioneers in this field were the british duo Pask and Beer which studied how
unstructured matter could be used to perform computational tasks.
In the introduction Toffolis statement that computation does not make sense
except in the light of evolution seems to contradict this notion of computation,
but evolution is not reserved only for nature.

% TODO: Cite
In one experiment [cite ???] the duo used silver in an acidic solution which would form
short-lived silver filaments when subjected to electric currents.
From our perspective of computation, it was not before they tuned the parameters
of the system in order to evolve a tone discriminator that the system could
truly be classified as a computing one.
Evolution in materio represents a very different approach than conventional
processors.
While it is impossible to program unstructured matter with imperative
instructions like in a conventional computer, we can nonetheless ``instruct''
the material computer in a declarative matter by specifying what sort of result
we are after and letting the evolutionary process handle the rest.
Material computing gives us a much better model of how neurons can compute:
Material computation happens on a massively parallel scale, it occurs in a
substrate where all structure is self-organized rather than imposed by a
designer, and both are products of evolution.\\

TODO: Write about Odd Rune's stuff, clean up and add citations.

\section{Neurons As Computers}
It might seem that the previous section has provided the key to unlocking the
computational power of neural cultures.
This line of thinking neglects the fact that neurons have already been shaped by
the process of evolution over billions of years, indeed neural cultures can be
viewed as the result of an EiM experiment a billion years in the making.
Our goal is not to apply the principle of EiM to neural cultures, but it does
provide an interesting angle:
When Pask and Beer tweaked their silver solution to discriminate tones, they did
viewed the solution as a black box, and only its performance on the task at hand
was evaluated.
While evolution does not optimize for a specific functionality the black box
approach is similar.
Just like Pask and Beer did not consider the exact inner workings of silver
filaments, it is necessary to apply the black box liberally when approaching
neurons.

% I really like the gist of this, but I'm very unsure about the phrasing.
If the neuron was the result of an EiM experiment, the ideal model of the neuron
would be the criteria used to evaluate fitness, eschewing all implementation
details.
Of course, no such criterias exist, evolution does not have a plan, but it
illustrates why as much as possible of the cells inner working will be
considered to be inside a black box.

% Another thing missing is of course how to utilize neurons.

\subsection{Neurons}
The neuron, or nerve cell, is the basic building block of both the human brain
and the nerve system.
There are many types of neurons in the human body, but to reduce the scope the
focus of this thesis is a simplified model of the neurons that make up the
brain.
The model neuron, shown in fig [a figure of a neuron] consists of three parts:
The body, \emph{Soma}, the \emph{Dendritic network} and an \emph{Axon}.
The dendritic network acts as a receptor sensing electrical activity around the
neuron, while the axon transmits electric pulses to neighboring cells.
The connection between two neurons is called a \emph{Synapse}.
Axons and dendritic networks are themselves vastly complex, and viewing them as
simple electrical transmitters neglects the importance of neurotransmitters.
However, given the strong correlation between neurotransmitter concentrations and
electrical activity is considered part of the black box.
When the neuron is sufficiently excited by electrical activity it will fire an
electrical pulse that travels along the axon, which in turn stimulate other
neurons.
Together neurons form networks in a complex interplay between topology and behavior:
The behavior of the network decides its behavior, and the behavior in turn
causes some synapses to wither, and others to form in a process that is not well
understood.
The model used for the neuron for the rest of this paper is a node in an
\emph{Adaptive Network} which communicates through electrical pulses.
The missing part in our model is the underlying rules that dictates the growth
of the network.
% What about spike trains?
\section{Reservoir Computing}
So far we have arrived at a model of the neuron as a complex adaptive network
which can be interfaced with using electrical signals, but the fundamental issue
of actually utilizing neurons for computing remains.
%
The final piece of the puzzle comes in the form of \emph{Reservoir Computing}, a
technique developed to exploit the dynamics of complex systems.
If the fundamental rule governing the neuron is to create networks exhibiting
the same complexity behavior as Langton's automatas then harnessing the
computational capabilities of these simple models is a first step towards
interfacing and understanding neurons.
%
% Clearly designing a cellular computer in a top down manner is intractable due to
% the intricate and unpredictable relation between cause and effect.
% The best we could realistically achieve with the typical top down approach is
% implementing a turing machine which would be both slow and incredibly brittle.
% Seemingly, classifying the neural culture as a complex system has not provided
% any useful tools for understanding how to interact with it, on the other hand it
% makes this task seem futile.
% However, in computer science the recent field of
% \emph{reservoir computing} has emerged, embracing the complexity and unpredictability
% of certain complex systems.
In reservoir computing, a complex systems is used as a \textit{reservoir}
\cite{schrauwen_overview_2007} which
``acts as a complex nonlinear dynamic filter that transforms the
input signals using a high-dimensional temporal map, not unlike the operation
of an explicit, temporal kernel function.''\\
% cite on SVMs?
In order to explain, schrauwen makes a comparison to the the machine learning
technique of source vector machines work, as shown in fig [rm 1]:
The reservoir acts as a kernel, projecting input into a high-dimensional feature space.
Figure [rm 1] shows this technique, note that the regression performed upon the
feauture space is a simple linear regression, an important point both in SVMs
and reservoir computing.
%
Figure [rm 2] shows a typical reservoir computing setup which follows a similar
method of operation as the SVM in figure [rm 1]
The reservoir serves as the high dimensional feature space, while the output
layer is only capable of linearly separating the resulting dynamics.
%
Schrauwen points out two major differences between SVMs and RCs.
First, SVMs only implicitly expands the input to high dimensional space in order
to make the problem tractable, while reservoirs do not.
Secondly, kernels are not capable of handling temporal signals.
%
The second difference is very important, it is what allows reservoirs to
implicitly encode temporal signals in their dynamics, making reservoirs a
natural fit for tasks such as speech recognition.
% cite Biologically Plausible Speech Recognition with LSTM Neural Nets Alex
% Graves, Douglas Eck, Nicole Beringer, Juergen Schmidhuber
% 
In other terms, the properties that make complex systems so hard to work with
such as sensitivity to initial conditions also allow them to discern very subtle
nuances in input, and their complex behavioral patterns causes the systems to
change their behavior to new input based on previous input.
%
In light of this, asking how to build a computer using Langton's automatons is
the wrong question, instead the focus should be on how exploit the computation
that is already occuring.\\
There are many examples of reservoirs which have been successfully exploited:
In \cite{jaeger_adaptive_2003} an \textit{echo state network} 
is utilized to solve classification problems.
More esoteric reservoirs have been used, for instance in
\cite{natschlager_liquid_2002} the idea of reservoir computing is taken quite
literally using a bucket of water as a reservoir.\\

\subsection{Linear and nonlinear output layers}
TODO: Elaborate on the use of linear vs nonlin classifiers.

\cleardoublepage

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End: